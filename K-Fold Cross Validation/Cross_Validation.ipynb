{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fDwhiQMQB4RX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KY8YynOsTcz"
      },
      "source": [
        "MNIST 데이터를 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBMsQiz7dVf_",
        "outputId": "8db91bef-801f-4746-d959-3530abb87ae5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60000\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor()\n",
        "])\n",
        "train_dataset = datasets.MNIST('./MNIST', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./MNIST', train=False, download=True, transform=transform)\n",
        "\n",
        "len_train = len(train_dataset)\n",
        "print(len_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6bZtcb4sWvc"
      },
      "source": [
        "모델을 설계한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C66O0rqoeGO1"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,h1=96):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # input: 1*28*28 \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # ouput: 16*14*14\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16*14*14, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)   \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG25HLd3shtV"
      },
      "source": [
        "GPU 사용을 위해 device를 초기화하고 random_seed를 지정한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qAVfxzqLg_Az"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(1)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed(1)\n",
        "  torch.cuda.manual_seed_all(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vkhbRIFsnw9"
      },
      "source": [
        "K-Fold Cross Validation을 위해서 코드를 KFold를 import하고 객체를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lbhsh7kchQLF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "splits = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "fold_dict = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSv3AcS7swTa"
      },
      "source": [
        "train과정에서 사용할 함수를 선언한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y8uTzfseiOVH"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, device, dataloader, loss_fn, optimizer):\n",
        "  train_loss, train_correct = 0.0, 0\n",
        "  model.train()\n",
        "  for images, labels in dataloader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()                                 # optimizer의 모든 파라미터들의 grad를 초기화한다.\n",
        "    hypothesis = model(images)                            # images를 model에 넣어주고 예측한다.\n",
        "    loss = loss_fn(hypothesis, labels)                    # loss값을 구한다.\n",
        "    loss.backward()                                       # loss값에 대해 미분을 수행한다.\n",
        "    optimizer.step()                                      # 학습을 진행한다. (optimizer를 한 단계 수행한다.)\n",
        "    train_loss += loss.item() * images.size(0)            # loss값에 image의 개수를 곱하고 저장한다.\n",
        "    scores, predictions = torch.max(hypothesis.data, 1)   # max로 가장 높게 예측한 값의 인덱스와 값(정수)를 뽑아준다.\n",
        "    train_correct += (predictions == labels).sum().item() # 인덱스가 label과 맞는지 확인하고 합을 구해서 맞힌 개수를 저장한다.\n",
        "  \n",
        "  return train_loss, train_correct                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viXQemmrvV7e"
      },
      "source": [
        "validation을 위한 함수다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MUrcuBHMnnql"
      },
      "outputs": [],
      "source": [
        "# train 참고\n",
        "def valid_epoch(model, device, dataloader, loss_fn):\n",
        "  valid_loss, val_correct = 0.0, 0\n",
        "  model.eval()\n",
        "  for images, labels in dataloader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      prediction = model(images)\n",
        "      loss=loss_fn(prediction,labels)\n",
        "      valid_loss+=loss.item()*images.size(0)\n",
        "      scores, predictions = torch.max(prediction.data,1)\n",
        "      val_correct+=(predictions == labels).sum().item()\n",
        "\n",
        "  return valid_loss,val_correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy5s8ROzlRNc"
      },
      "source": [
        "각 Fold를 각각 3번씩 epoch시켜서 학습시켜보자. <br>\n",
        "1-Fold 5번, 2-Fold 5번, ..., 5-Fold 5번 식으로 진행된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vn-h-JwvVaG",
        "outputId": "210d8c7f-2af8-4ebb-f96f-e6270aacc1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "[Epoch:1/5] AVG Training Loss/Acc: 0.130/96.16, AVG Test Loss/Acc: 0.073/97.91\n",
            "[Epoch:2/5] AVG Training Loss/Acc: 0.038/98.92, AVG Test Loss/Acc: 0.054/98.34\n",
            "[Epoch:3/5] AVG Training Loss/Acc: 0.021/99.39, AVG Test Loss/Acc: 0.060/98.18\n",
            "[Epoch:4/5] AVG Training Loss/Acc: 0.011/99.74, AVG Test Loss/Acc: 0.054/98.44\n",
            "[Epoch:5/5] AVG Training Loss/Acc: 0.008/99.78, AVG Test Loss/Acc: 0.065/98.14\n",
            "Fold 2\n",
            "[Epoch:1/5] AVG Training Loss/Acc: 0.137/96.00, AVG Test Loss/Acc: 0.062/98.19\n",
            "[Epoch:2/5] AVG Training Loss/Acc: 0.042/98.78, AVG Test Loss/Acc: 0.055/98.15\n",
            "[Epoch:3/5] AVG Training Loss/Acc: 0.022/99.34, AVG Test Loss/Acc: 0.052/98.43\n",
            "[Epoch:4/5] AVG Training Loss/Acc: 0.012/99.67, AVG Test Loss/Acc: 0.048/98.57\n",
            "[Epoch:5/5] AVG Training Loss/Acc: 0.008/99.77, AVG Test Loss/Acc: 0.053/98.38\n",
            "Fold 3\n",
            "[Epoch:1/5] AVG Training Loss/Acc: 0.134/96.11, AVG Test Loss/Acc: 0.067/97.96\n",
            "[Epoch:2/5] AVG Training Loss/Acc: 0.041/98.79, AVG Test Loss/Acc: 0.053/98.40\n",
            "[Epoch:3/5] AVG Training Loss/Acc: 0.019/99.49, AVG Test Loss/Acc: 0.051/98.47\n",
            "[Epoch:4/5] AVG Training Loss/Acc: 0.012/99.66, AVG Test Loss/Acc: 0.050/98.58\n",
            "[Epoch:5/5] AVG Training Loss/Acc: 0.007/99.82, AVG Test Loss/Acc: 0.047/98.66\n",
            "Fold 4\n",
            "[Epoch:1/5] AVG Training Loss/Acc: 0.133/96.11, AVG Test Loss/Acc: 0.058/98.28\n",
            "[Epoch:2/5] AVG Training Loss/Acc: 0.043/98.74, AVG Test Loss/Acc: 0.042/98.64\n",
            "[Epoch:3/5] AVG Training Loss/Acc: 0.024/99.27, AVG Test Loss/Acc: 0.039/98.90\n",
            "[Epoch:4/5] AVG Training Loss/Acc: 0.013/99.62, AVG Test Loss/Acc: 0.042/98.64\n",
            "[Epoch:5/5] AVG Training Loss/Acc: 0.007/99.83, AVG Test Loss/Acc: 0.040/98.78\n",
            "Fold 5\n",
            "[Epoch:1/5] AVG Training Loss/Acc: 0.132/96.16, AVG Test Loss/Acc: 0.065/98.07\n",
            "[Epoch:2/5] AVG Training Loss/Acc: 0.040/98.82, AVG Test Loss/Acc: 0.058/98.28\n",
            "[Epoch:3/5] AVG Training Loss/Acc: 0.020/99.45, AVG Test Loss/Acc: 0.053/98.37\n",
            "[Epoch:4/5] AVG Training Loss/Acc: 0.011/99.70, AVG Test Loss/Acc: 0.049/98.62\n",
            "[Epoch:5/5] AVG Training Loss/Acc: 0.008/99.81, AVG Test Loss/Acc: 0.051/98.58\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
        "    print('Fold {}'.format(fold + 1))\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = ConvNet()\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    history = {'train_loss': [], 'valid_loss': [],'train_acc':[],'valid_acc':[]}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
        "        valid_loss, val_correct=valid_epoch(model,device,valid_loader,criterion)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.sampler)\n",
        "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
        "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "        valid_acc = val_correct / len(valid_loader.sampler) * 100\n",
        "\n",
        "        print(f\"[Epoch:{epoch+1}/{num_epochs}] AVG Training Loss/Acc: {train_loss:.3f}/{train_acc:.2f}, AVG Test Loss/Acc: {valid_loss:.3f}/{valid_acc:.2f}\")\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['valid_acc'].append(valid_acc)\n",
        "\n",
        "    fold_dict['fold{}'.format(fold+1)] = history\n",
        "\n",
        "torch.save(model,'k_cross_CNN.pt')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9t_mjLvmNiK"
      },
      "source": [
        "fold_dict에는 다음과 같은 값들이 저장된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swOC6hSHlQi5",
        "outputId": "7cc4aeb1-a628-4298-8a41-551d7b35b44e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<fold1>\n",
            "train_loss: [0.1299572738458713, 0.03798096673190594, 0.02108794889661173, 0.011165055048496773, 0.008147445250302553]\n",
            "valid_loss: [0.07254208528995514, 0.05435066587726275, 0.06025160567710797, 0.053579377385477224, 0.06464485293378433]\n",
            "train_acc: [96.16458333333333, 98.92291666666667, 99.38749999999999, 99.73541666666667, 99.77916666666667]\n",
            "valid_acc: [97.90833333333333, 98.34166666666667, 98.18333333333334, 98.44166666666668, 98.14166666666667]\n"
          ]
        }
      ],
      "source": [
        "for key, values in fold_dict.items():\n",
        "  print(f'<{key}>')\n",
        "  for key, values in values.items():\n",
        "    print(f\"{key}: {values}\")\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JULw-AI7mc2Y"
      },
      "source": [
        "5-Fold cross validation의 성능을 확인해보자."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0hF9_rhmS8m",
        "outputId": "b804e809-558e-4f28-98d8-dbb9b9d337e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance of 5-Fold Cross Validation (Train Set)\n",
            "Training Loss/Acc: 0.043/98.76, Test Loss/Acc: 0.054/98.40\n"
          ]
        }
      ],
      "source": [
        "train_loss, train_acc, valid_loss, valid_acc = [], [], [], []\n",
        "\n",
        "k = 5\n",
        "for i in range(k):\n",
        "  train_loss.append(np.mean(fold_dict[f'fold{i+1}']['train_loss']))\n",
        "  train_acc.append(np.mean(fold_dict[f'fold{i+1}']['train_acc']))\n",
        "  \n",
        "  valid_loss.append(np.mean(fold_dict[f'fold{i+1}']['valid_loss']))\n",
        "  valid_acc.append(np.mean(fold_dict[f'fold{i+1}']['valid_acc']))\n",
        "\n",
        "print(f'Performance of {k}-Fold Cross Validation (Train Set)')\n",
        "print(f\"Training Loss/Acc: {np.mean(train_loss):.3f}/{np.mean(train_acc):.2f}, Test Loss/Acc: {np.mean(valid_loss):.3f}/{np.mean(valid_acc):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7Lb7HXYuO6J"
      },
      "source": [
        "이후 이 모델들을 앙상블시키면 더 좋은 모델을 얻을 수도 있다고 한다. <br>\n",
        "앙상블은 이후에 적용해보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkZPZM14ojb3"
      },
      "source": [
        "## <strong> 알게 된 점 </strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PcyZK73os2n"
      },
      "source": [
        "<h3> <strong> 1. tensor.data </strong> </h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqMRcNyQo8j4"
      },
      "source": [
        "tensor data에 data 속성을 사용하면 해당 데이터를 반환해준다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSlZhwhgpG8N",
        "outputId": "c8f625a8-cc3c-4fb8-f735-6cd2ddcc255d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2., 3., 4., 5.],\n",
              "        [1., 2., 3., 4., 5.],\n",
              "        [1., 2., 3., 4., 5.],\n",
              "        [1., 2., 3., 4., 5.]])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.Tensor([[1, 2, 3, 4, 5],\n",
        "                 [1, 2, 3, 4, 5],\n",
        "                 [1, 2, 3, 4, 5],\n",
        "                 [1, 2, 3, 4, 5]])\n",
        "a.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8VnGCNIplPH"
      },
      "source": [
        "<h3> <strong> 2. tensor.item() </strong> </h3>\n",
        "\n",
        "tensor.item()을 사용하면 tensor내의 값만 출력할 수 있다. <br>\n",
        "단, 하나의 원소만 가진 tensor여야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGgsFZ1RpuFe",
        "outputId": "1ff54c3a-5865-48f0-e4f7-4db20dd3727f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor: tensor([0.1000])\n",
            "tensor.item: 0.10000000149011612\n"
          ]
        }
      ],
      "source": [
        "a = torch.FloatTensor([0.1])\n",
        "# a = torch.FloatTensor([0.1, 0.2]) 사용시 에러 발생\n",
        "print(f'tensor: {a}')\n",
        "print(f'tensor.item: {a.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuvtJMQFUTeS"
      },
      "source": [
        "<h3> <strong> 3. DataLoader parameter-sampler </strong> </h3>\n",
        "\n",
        "sampler는 index를 컨트롤하는 방법이다. 데이터의 index를 원하는 방식대로 조정한다. <br>\n",
        "index를 컨트롤해야 하므로 **shuffle 파라미터는 False**여야 한다.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNhbaaEeVnt5",
        "outputId": "2e19f573-5bf0-4062-d37f-5bd3c32db1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 Fold: [0 1 3 4 5 6 7 8] / [2 9]\n",
            "<train_loader>\n",
            "len(train_loader.sampler): 8\n",
            "[tensor([[3., 3., 3.],\n",
            "        [4., 4., 4.]])]\n",
            "[tensor([[8., 8., 8.],\n",
            "        [0., 0., 0.]])]\n",
            "[tensor([[7., 7., 7.],\n",
            "        [1., 1., 1.]])]\n",
            "[tensor([[6., 6., 6.],\n",
            "        [5., 5., 5.]])]\n",
            "<valid_loader>\n",
            "len(valid_loader.sampler): 2\n",
            "[tensor([[9., 9., 9.],\n",
            "        [2., 2., 2.]])]\n",
            "\n",
            "2 Fold: [0 1 2 3 5 7 8 9] / [4 6]\n",
            "<train_loader>\n",
            "len(train_loader.sampler): 8\n",
            "[tensor([[0., 0., 0.],\n",
            "        [7., 7., 7.]])]\n",
            "[tensor([[1., 1., 1.],\n",
            "        [9., 9., 9.]])]\n",
            "[tensor([[2., 2., 2.],\n",
            "        [3., 3., 3.]])]\n",
            "[tensor([[5., 5., 5.],\n",
            "        [8., 8., 8.]])]\n",
            "<valid_loader>\n",
            "len(valid_loader.sampler): 2\n",
            "[tensor([[6., 6., 6.],\n",
            "        [4., 4., 4.]])]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "x = torch.FloatTensor([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4],\n",
        "                       [5, 5, 5], [6, 6, 6], [7, 7, 7], [8, 8, 8], [9, 9, 9]])\n",
        "\n",
        "x_dataset = TensorDataset(x) # dataset 생성\n",
        "splits = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "\n",
        "# np.arange(len(x_dataset)) -> 모든 인덱스를 담은 np array를 생성한다.\n",
        "for fold, (train_idx, valid_idx) in enumerate(splits.split(np.arange(len(x_dataset)))):\n",
        "  print(f'{fold+1} Fold: {train_idx} / {valid_idx}') \n",
        "  train_sampler = SubsetRandomSampler(train_idx)\n",
        "  valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "  train_loader = DataLoader(x_dataset, batch_size=2, sampler=train_sampler)\n",
        "  valid_loader = DataLoader(x_dataset, batch_size=2, sampler=valid_sampler)\n",
        "  \n",
        "  print('<train_loader>')\n",
        "  print(f'len(train_loader.sampler): {len(train_loader.sampler)}')\n",
        "  for i in train_loader:\n",
        "    print(i)\n",
        "\n",
        "  print('<valid_loader>')\n",
        "  print(f'len(valid_loader.sampler): {len(valid_loader.sampler)}')\n",
        "  for i in valid_loader:\n",
        "    print(i)\n",
        "  \n",
        "  print()\n",
        "\n",
        "  if fold == 1: break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Cross_Validation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
