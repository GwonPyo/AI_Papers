{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Early Stopping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "일반적인 validation을 수행할 경우 아래 코드와 같이 수행하면 된다."
      ],
      "metadata": {
        "id": "pU_6kjB2B_e1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UyKZdY0KVrb"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
        "        self.patience = patience                    # 학습이 개선되지 않을 경우 참을 횟수\n",
        "        self.verbose = verbose                      # 학습 개선 메세지를 출력할지 여부 결정\n",
        "        self.counter = 0                            # 학습이 개선되지 않은 횟수\n",
        "        self.best_score = None                      # 지금까지 가장 최적의 loss\n",
        "        self.early_stop = False                     # early_stop을 해야하는 경우 True로 값 변경\n",
        "        self.val_loss_min = np.Inf                  \n",
        "        self.delta = delta                          # 최소한의 loss값 개선 수준\n",
        "        self.path = path                            # 모델을 저장할 주소\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss                           # loss값을 음수로 바꾼다.\n",
        "\n",
        "        if self.best_score is None:                 # best_score에 score가 저장된 적이 없다면 \n",
        "            self.best_score = score                 # 해당값을 best_score에 저장하고\n",
        "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
        "        elif score < self.best_score + self.delta:  # best_score에 delta를 더한 값보다 score가 작다면 == 이전 최대 점수에 delta를 뺀 값보다 score가 크다면 (delta값은 어느 정도의 학습을 강제하는 느낌인 것 같다.)\n",
        "            self.counter += 1                       # counter값을 갱신하고\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:       # counter값이 patience보다 크다면\n",
        "                self.early_stop = True              # early_stop을 True로 갱신한다.\n",
        "        else:\n",
        "            self.best_score = score                 # 만약 loss값이 알맞게 감소했다면\n",
        "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
        "            self.counter = 0                        # counter값도 다시 0으로 초기화한다.\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), self.path)\n",
        "        self.val_loss_min = val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cross-validation일 경우 아래 코드를 수행한다."
      ],
      "metadata": {
        "id": "2zlIfEuZCF6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', model_name = None, fold=None):\n",
        "        self.fold = fold\n",
        "        self.model_name = model_name\n",
        "        \n",
        "        self.patience = patience                    # 학습이 개선되지 않을 경우 참을 횟수\n",
        "        self.verbose = verbose                      # 학습 개선 메세지를 출력할지 여부 결정\n",
        "        self.counter = 0                            # 학습이 개선되지 않은 횟수\n",
        "        self.best_score = None                      # 지금까지 가장 최적의 loss\n",
        "        self.early_stop = False                     # early_stop을 해야하는 경우 True로 값 변경\n",
        "        self.val_loss_min = np.Inf                  \n",
        "        self.delta = delta                          # 최소한의 loss값 개선 수준\n",
        "        self.path = path                            # 모델을 저장할 주소\n",
        "        \n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss                           # loss값을 음수로 바꾼다.\n",
        "\n",
        "        if self.best_score is None:                 # best_score에 score가 저장된 적이 없다면 \n",
        "            self.best_score = score                 # 해당값을 best_score에 저장하고\n",
        "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
        "        elif score < self.best_score + self.delta:  # best_score에 delta를 더한 값보다 score가 작다면 == 이전 최대 점수에 delta를 뺀 값보다 score가 크다면 (delta값은 어느 정도의 학습을 강제하는 느낌인 것 같다.)\n",
        "            self.counter += 1                       # counter값을 갱신하고\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:       # counter값이 patience보다 크다면\n",
        "                self.early_stop = True              # early_stop을 True로 갱신한다.\n",
        "        else:\n",
        "            self.best_score = score                 # 만약 loss값이 알맞게 감소했다면\n",
        "            self.save_checkpoint(val_loss, model)   # loss값과 model을 저장한다.\n",
        "            self.counter = 0                        # counter값도 다시 0으로 초기화한다.\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        if self.fold is not None and self.model_name is not None:\n",
        "            torch.save(model.state_dict(), self.model_name + '_'+ str(self.fold)+'fold_'+self.path)\n",
        "        else:\n",
        "            torch.save(model.state_dict(),self.path)\n",
        "        self.val_loss_min = val_loss"
      ],
      "metadata": {
        "id": "no_8Mb6aB-_Q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 출처: https://quokkas.tistory.com/37"
      ],
      "metadata": {
        "id": "i1EempYGEjxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <strong> 적용 </strong>"
      ],
      "metadata": {
        "id": "JvSPAWOgHbay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이전에 Cross Validation 코드를 가져와 early stopping을 적용해보자."
      ],
      "metadata": {
        "id": "hwbDIsGgHf0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "i6CaubihHl-8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST 데이터를 load해준다."
      ],
      "metadata": {
        "id": "n_dbM1RzULK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "                                transforms.ToTensor()\n",
        "])\n",
        "train_dataset = datasets.MNIST('./MNIST', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./MNIST', train=False, download=True, transform=transform)\n",
        "\n",
        "len_train = len(train_dataset)\n",
        "print(len_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYwQZe5OH7Fh",
        "outputId": "f062a3f4-8182-49c0-ebb0-b0f72d50f04e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습시킬 모델을 구성한다."
      ],
      "metadata": {
        "id": "lqO0Yf-hUNyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self,h1=96):\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # input: 1*28*28 \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # ouput: 16*14*14\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(16*14*14, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)   \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "KncBpxVhHrd9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gpu를 설정한다."
      ],
      "metadata": {
        "id": "4exfKxAYUSu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(1)\n",
        "if device == 'cuda':\n",
        "  torch.cuda.manual_seed(1)\n",
        "  torch.cuda.manual_seed_all(1)"
      ],
      "metadata": {
        "id": "3jYM2C00HtpV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "교차 검증을 위해 KFold 객체를 선언한다."
      ],
      "metadata": {
        "id": "wgQI3mKJUU8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "splits = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "fold_dict = {}"
      ],
      "metadata": {
        "id": "OJ0bPskBHvEW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train, validation의 각 epoch에 사용할 함수들을 정의한다."
      ],
      "metadata": {
        "id": "d_Oim3OZUZT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, device, dataloader, loss_fn, optimizer):\n",
        "  train_loss, train_correct = 0.0, 0\n",
        "  model.train()\n",
        "  for images, labels in dataloader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()                                 # optimizer의 모든 파라미터들의 grad를 초기화한다.\n",
        "    hypothesis = model(images)                            # images를 model에 넣어주고 예측한다.\n",
        "    loss = loss_fn(hypothesis, labels)                    # loss값을 구한다.\n",
        "    loss.backward()                                       # loss값에 대해 미분을 수행한다.\n",
        "    optimizer.step()                                      # 학습을 진행한다. (optimizer를 한 단계 수행한다.)\n",
        "    train_loss += loss.item() * images.size(0)            # loss값에 image의 개수를 곱하고 저장한다.\n",
        "    scores, predictions = torch.max(hypothesis.data, 1)   # max로 가장 높게 예측한 값의 인덱스와 값(정수)를 뽑아준다.\n",
        "    train_correct += (predictions == labels).sum().item() # 인덱스가 label과 맞는지 확인하고 합을 구해서 맞힌 개수를 저장한다.\n",
        "  \n",
        "  return train_loss, train_correct"
      ],
      "metadata": {
        "id": "TGLHEPT6HwPR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 참고\n",
        "def valid_epoch(model, device, dataloader, loss_fn):\n",
        "  valid_loss, val_correct = 0.0, 0\n",
        "  model.eval()\n",
        "  for images, labels in dataloader:\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      prediction = model(images)\n",
        "      loss=loss_fn(prediction,labels)\n",
        "      valid_loss+=loss.item()*images.size(0)\n",
        "      scores, predictions = torch.max(prediction.data,1)\n",
        "      val_correct+=(predictions == labels).sum().item()\n",
        "\n",
        "  return valid_loss,val_correct"
      ],
      "metadata": {
        "id": "C5aPobnlHxoZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습시킨다."
      ],
      "metadata": {
        "id": "1CscUtprUdlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "\n",
        "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(train_dataset)))):\n",
        "    print('Fold {}'.format(fold+1))\n",
        "    early_stopping = EarlyStopping(patience=2, verbose=True, fold=fold+1, model_name = 'ConvNet')\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    valid_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "    \n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    model = ConvNet()\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    history = {'train_loss': [], 'valid_loss': [],'train_acc':[],'valid_acc':[]}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_correct=train_epoch(model,device,train_loader,criterion,optimizer)\n",
        "        valid_loss, val_correct=valid_epoch(model,device,valid_loader,criterion)\n",
        "\n",
        "        train_loss = train_loss / len(train_loader.sampler)\n",
        "        train_acc = train_correct / len(train_loader.sampler) * 100\n",
        "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
        "        valid_acc = val_correct / len(valid_loader.sampler) * 100\n",
        "\n",
        "        print(f\"[Epoch:{epoch+1}/{num_epochs}] AVG Training Loss/Acc: {train_loss:.3f}/{train_acc:.2f}, AVG Test Loss/Acc: {valid_loss:.3f}/{valid_acc:.2f}\")\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['valid_loss'].append(valid_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['valid_acc'].append(valid_acc)\n",
        "\n",
        "        early_stopping(valid_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    fold_dict['fold{}'.format(fold+1)] = history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSxKzjFmHzE0",
        "outputId": "06431e29-b4ae-4d69-e26b-38cdc175cc6f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1\n",
            "[Epoch:1/10] AVG Training Loss/Acc: 0.131/96.24, AVG Test Loss/Acc: 0.065/98.07\n",
            "Validation loss decreased (inf --> 0.065480).  Saving model ...\n",
            "[Epoch:2/10] AVG Training Loss/Acc: 0.041/98.72, AVG Test Loss/Acc: 0.058/98.30\n",
            "Validation loss decreased (0.065480 --> 0.058408).  Saving model ...\n",
            "[Epoch:3/10] AVG Training Loss/Acc: 0.021/99.42, AVG Test Loss/Acc: 0.053/98.40\n",
            "Validation loss decreased (0.058408 --> 0.052824).  Saving model ...\n",
            "[Epoch:4/10] AVG Training Loss/Acc: 0.013/99.65, AVG Test Loss/Acc: 0.062/98.21\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:5/10] AVG Training Loss/Acc: 0.010/99.71, AVG Test Loss/Acc: 0.058/98.41\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n",
            "Fold 2\n",
            "[Epoch:1/10] AVG Training Loss/Acc: 0.128/96.28, AVG Test Loss/Acc: 0.065/97.99\n",
            "Validation loss decreased (inf --> 0.064749).  Saving model ...\n",
            "[Epoch:2/10] AVG Training Loss/Acc: 0.040/98.82, AVG Test Loss/Acc: 0.054/98.32\n",
            "Validation loss decreased (0.064749 --> 0.053900).  Saving model ...\n",
            "[Epoch:3/10] AVG Training Loss/Acc: 0.020/99.41, AVG Test Loss/Acc: 0.050/98.41\n",
            "Validation loss decreased (0.053900 --> 0.049659).  Saving model ...\n",
            "[Epoch:4/10] AVG Training Loss/Acc: 0.011/99.70, AVG Test Loss/Acc: 0.052/98.33\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:5/10] AVG Training Loss/Acc: 0.008/99.79, AVG Test Loss/Acc: 0.055/98.39\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n",
            "Fold 3\n",
            "[Epoch:1/10] AVG Training Loss/Acc: 0.140/95.88, AVG Test Loss/Acc: 0.072/97.72\n",
            "Validation loss decreased (inf --> 0.072460).  Saving model ...\n",
            "[Epoch:2/10] AVG Training Loss/Acc: 0.045/98.70, AVG Test Loss/Acc: 0.054/98.50\n",
            "Validation loss decreased (0.072460 --> 0.053749).  Saving model ...\n",
            "[Epoch:3/10] AVG Training Loss/Acc: 0.022/99.40, AVG Test Loss/Acc: 0.050/98.52\n",
            "Validation loss decreased (0.053749 --> 0.049843).  Saving model ...\n",
            "[Epoch:4/10] AVG Training Loss/Acc: 0.014/99.59, AVG Test Loss/Acc: 0.051/98.52\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:5/10] AVG Training Loss/Acc: 0.009/99.79, AVG Test Loss/Acc: 0.048/98.57\n",
            "Validation loss decreased (0.049843 --> 0.048089).  Saving model ...\n",
            "[Epoch:6/10] AVG Training Loss/Acc: 0.006/99.84, AVG Test Loss/Acc: 0.049/98.73\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:7/10] AVG Training Loss/Acc: 0.006/99.87, AVG Test Loss/Acc: 0.050/98.57\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n",
            "Fold 4\n",
            "[Epoch:1/10] AVG Training Loss/Acc: 0.146/95.74, AVG Test Loss/Acc: 0.063/98.03\n",
            "Validation loss decreased (inf --> 0.062753).  Saving model ...\n",
            "[Epoch:2/10] AVG Training Loss/Acc: 0.041/98.83, AVG Test Loss/Acc: 0.052/98.32\n",
            "Validation loss decreased (0.062753 --> 0.052183).  Saving model ...\n",
            "[Epoch:3/10] AVG Training Loss/Acc: 0.022/99.38, AVG Test Loss/Acc: 0.048/98.49\n",
            "Validation loss decreased (0.052183 --> 0.048260).  Saving model ...\n",
            "[Epoch:4/10] AVG Training Loss/Acc: 0.012/99.67, AVG Test Loss/Acc: 0.043/98.58\n",
            "Validation loss decreased (0.048260 --> 0.043158).  Saving model ...\n",
            "[Epoch:5/10] AVG Training Loss/Acc: 0.008/99.81, AVG Test Loss/Acc: 0.044/98.71\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:6/10] AVG Training Loss/Acc: 0.007/99.81, AVG Test Loss/Acc: 0.044/98.68\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n",
            "Fold 5\n",
            "[Epoch:1/10] AVG Training Loss/Acc: 0.132/96.13, AVG Test Loss/Acc: 0.058/98.39\n",
            "Validation loss decreased (inf --> 0.057794).  Saving model ...\n",
            "[Epoch:2/10] AVG Training Loss/Acc: 0.039/98.85, AVG Test Loss/Acc: 0.057/98.37\n",
            "Validation loss decreased (0.057794 --> 0.056809).  Saving model ...\n",
            "[Epoch:3/10] AVG Training Loss/Acc: 0.020/99.40, AVG Test Loss/Acc: 0.050/98.51\n",
            "Validation loss decreased (0.056809 --> 0.049831).  Saving model ...\n",
            "[Epoch:4/10] AVG Training Loss/Acc: 0.012/99.66, AVG Test Loss/Acc: 0.047/98.77\n",
            "Validation loss decreased (0.049831 --> 0.046755).  Saving model ...\n",
            "[Epoch:5/10] AVG Training Loss/Acc: 0.007/99.81, AVG Test Loss/Acc: 0.048/98.58\n",
            "EarlyStopping counter: 1 out of 2\n",
            "[Epoch:6/10] AVG Training Loss/Acc: 0.005/99.88, AVG Test Loss/Acc: 0.058/98.52\n",
            "EarlyStopping counter: 2 out of 2\n",
            "Early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습시켰던 모델들을 불러온다."
      ],
      "metadata": {
        "id": "eZOvgIKfUfs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = ConvNet(); model1.load_state_dict(torch.load('ConvNet_0fold_checkpoint.pt'))\n",
        "model2 = ConvNet(); model2.load_state_dict(torch.load('ConvNet_1fold_checkpoint.pt'))\n",
        "model3 = ConvNet(); model3.load_state_dict(torch.load('ConvNet_2fold_checkpoint.pt'))\n",
        "model4 = ConvNet(); model4.load_state_dict(torch.load('ConvNet_3fold_checkpoint.pt'))\n",
        "model5 = ConvNet(); model5.load_state_dict(torch.load('ConvNet_4fold_checkpoint.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAk9yf_XH_ry",
        "outputId": "3ce6bc1a-73b7-4355-abfd-5ee3a0c1b1d0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "oekveAg-QKEh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델의 성능을 확인한다. <br>\n",
        "학습시 추론은 각 모델이 추론한 값을 평균하여 구했다."
      ],
      "metadata": {
        "id": "4ITgvw3EUkuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(models):\n",
        "  for model in models:\n",
        "    model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    accuracy = 0\n",
        "    len_models = len(models)       # 모델 개수\n",
        "    total_data = len(test_dataset) # 전체 test_dataset 개수\n",
        "    for x, y in test_loader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "      prediction = 0\n",
        "      for model in models:\n",
        "        prediction +=  model(x) / len_models\n",
        "      \n",
        "      _, prediction = torch.max(prediction, 1)\n",
        "      correct_prediction = (prediction == y) \n",
        "      accuracy += torch.sum(correct_prediction) / total_data\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "N-UTK7mAOrXt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [model1, model2, model3, model4, model5]\n",
        "test(models).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjyozjhzTj1X",
        "outputId": "da755fdc-bb55-4654-92b2-c7de90055dbf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9899997115135193"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}